{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis Ready Data Tutorial Part 2: Use Case 1\n",
    "\n",
    "Time-series analysis (e.g. change detection and trend detection) is a powerful application of satellite imagery. However, a great deal of processing is required to prepare imagery for analysis. Analysis Ready Data (ARD), preprocessed time-series stacks of overhead imagery, allow for time-series analysis without any additional processing of the imagery. See [Analysis Data Defined](https://medium.com/planet-stories/analysis-ready-data-defined-5694f6f48815) for an excellent introduction and discussion on ARD.\n",
    "\n",
    "In [Part 1](ard_1_intro_and_best_practices.ipynb) of this tutorial, we introduced ARD and covered the how and whys of using the Data and Orders APIs to create and interpret ARD.\n",
    "\n",
    "This second part of the tutorial focuses on the first of two use cases. The use case addressed in this tutorial is:\n",
    "\n",
    "* As a software engineer at an ag-tech company, I'd like to be able to order Planet imagery programmatically in a way that enables the data scientist at my organization to create time-series algorithms (e.g. monitoring ndvi curves over time) without further data cleaning and processing.\n",
    "\n",
    "Please see the first part of the tutorial for an introduction to the Data and Orders APIs along with best practices. A lot of functionality developed in that tutorial will be copied here in a compact form.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Two things are interesting about this use case. First, we are calculating NDVI, and second, we are compositing scenes together. What is NDVI and what is compositing and why do we want to do it?\n",
    "\n",
    "Great questions!\n",
    "\n",
    "First, NDVI stands for normalized difference vegitation index. It is used a **LOT** to find out if vegetation is growing. You can find out more about NDVI at [USGS](https://www.usgs.gov/land-resources/eros/phenology/science/ndvi-foundation-remote-sensing-phenology?qt-science_center_objects=0#qt-science_center_objects) and [Wikipedia](https://en.wikipedia.org/wiki/Normalized_difference_vegetation_index). What we care about here is that NDVI uses the red and near-infrared bands of an image and returns one band with values that range from -1 to 1. So, we expect a single-band image for each order.\n",
    "\n",
    "Compositing is a way to stitch multiple scenes together for maximum coverage. We want this because for a time series, we just want one image for each date and we want that one image to have the most coverage to minimize holes in our data. The composite tool takes in multiple scenes and returns one image. If we feed it scenes from a whole timestack, we still just get one image back! So, to avoid that disaster, we group our scenes by date and only composite the scenes that were collected on the same date.\n",
    "\n",
    "\n",
    "## Implementation\n",
    "\n",
    "The use case we will cover is: *As a software engineer at an ag-tech company, I'd like to be able to order Planet imagery programmatically in a way that enables the data scientist at my organization to create time-series algorithms (e.g. monitoring ndvi curves over time) without further data cleaning and processing.*\n",
    "\n",
    "For this use case, the area of interest and time range are not specified. The need for no further processing indicates we should specify a strict usable pixel data filter. For time-series analysis the daily coverage of PS satellites is ideal. For our time-series analysis, we would like a single image that covers the entire area of interest (AOI). However, it may take multiple scenes to cover the entire AOI. Therefore, we will use the Composite tool to make a composite for each day in the time series analysis. This is a little tricky because the Composite tool just composites all of the scenes associated with the ids ordered. So we need to parse the scene ids we got from the Data API to get scene ids for each day, then submit an order for each day.\n",
    "\n",
    "To summarize, these are the steps:\n",
    "1. [Initialize API client](#Step-1:-Initialize-API-client)\n",
    "1. [Search Data API](#Step-2:-Search-Data-API)\n",
    "1. [Group IDs by Date](#Step-3:-Group-IDs-by-Date)\n",
    "1. [Submit Orders](#Step-4:-Submit-Orders)\n",
    "1. [Download Orders](#Step-5:-Download-Orders)\n",
    "1. [Unzip and Verify Orders](#Step-6:-Unzip-and-Verify-Orders)\n",
    "\n",
    "Note that, due to the processing-intensiveness of visualizing the NDVI images and UDM2s, we will be covering visualization in the next notebook, [Analysis Ready Data Tutorial Part 2: Use Case 1 - Visualization](ard_2_use_case_1_visualize_images.ipynb)\n",
    "\n",
    "#### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from copy import copy\n",
    "import datetime\n",
    "from itertools import chain\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import shutil\n",
    "import time\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import numpy as np\n",
    "from planet import Auth\n",
    "from planet import Session, DataClient, OrdersClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Initialize API client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if your Planet API Key is not set as an environment variable, you can paste it below\n",
    "API_KEY = os.environ.get('PL_API_KEY', 'PASTE_YOUR_KEY_HERE')\n",
    "\n",
    "client = Auth.from_key(API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Search Data API\n",
    "\n",
    "The goal of this step is to get the scene ids that meet the search criteria for this use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define test data for the filter\n",
    "\n",
    "# iowa crops aoi\n",
    "test_aoi_geom = {\n",
    "    \"type\": \"Polygon\",\n",
    "    \"coordinates\": [\n",
    "        [\n",
    "            [-93.299129, 42.699599],\n",
    "            [-93.299674, 42.812757],\n",
    "            [-93.288436, 42.861921],\n",
    "            [-93.265332, 42.924817],\n",
    "            [-92.993873, 42.925124],\n",
    "            [-92.993888, 42.773637],\n",
    "            [-92.998396, 42.754529],\n",
    "            [-93.019154, 42.699988],\n",
    "            [-93.299129, 42.699599]\n",
    "        ]\n",
    "    ]\n",
    "}\n",
    "\n",
    "### Let's search:\n",
    "# for the geometry above\n",
    "# a PSScene image\n",
    "# Date Range: April 1st - May 1st 2019\n",
    "# Clear Percent: 90% or above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an API Request from the search specifications\n",
    "\n",
    "item_type = ['PSScene']\n",
    "\n",
    "geom_filter = {\n",
    "   \"type\":\"GeometryFilter\",\n",
    "   \"field_name\":\"geometry\",\n",
    "   \"config\":test_aoi_geom\n",
    "}\n",
    "\n",
    "clear_percent_filter = {\n",
    "\"type\":\"RangeFilter\",\n",
    "\"field_name\":\"clear_percent\",\n",
    "\"config\":{\n",
    "  \"gte\":0.9}\n",
    "}\n",
    "\n",
    "date_range_filter = {\n",
    "\"type\":\"DateRangeFilter\",\n",
    "\"field_name\":\"acquired\",\n",
    "\"config\":{\n",
    "  \"gt\":\"2019-04-01T00:00:00Z\", \n",
    "   \"lt\": \"2019-05-01T00:00:00Z\"}\n",
    "}\n",
    "\n",
    "combined_filter = {\n",
    "\"type\":\"AndFilter\",\n",
    "\"config\":[\n",
    "    geom_filter,\n",
    "    date_range_filter,\n",
    "    clear_percent_filter]\n",
    "}\n",
    "\n",
    "async with Session() as sess:\n",
    "    cl = DataClient(sess)\n",
    "    request = await cl.create_search(name='temp_search2',search_filter=combined_filter, item_types=item_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at our search request.\n",
    "# Note: This is just the request's structure, the search hasn't been implemented yet\n",
    "request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search the Data API\n",
    "async with Session() as sess:\n",
    "    cl = DataClient(sess)\n",
    "    items = await cl.run_search(search_id=request['id'])\n",
    "    item_list = [i async for i in items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(item_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Group IDs by Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check out an item just for fun\n",
    "print(item_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's grab this first item in our list and look at the date it was acquired\n",
    "item = item_list[0]\n",
    "acquired_date = item['properties']['acquired'].split('T')[0]\n",
    "acquired_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can create a function to get the acquired dates for all of our search results\n",
    "def get_acquired_date(item):\n",
    "    return item['properties']['acquired'].split('T')[0]\n",
    "\n",
    "acquired_dates = [get_acquired_date(item) for item in item_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the unique values of Acquired Date for our results\n",
    "unique_acquired_dates = set(acquired_dates)\n",
    "unique_acquired_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We can also list our Image IDs grouped based on Acquired Date\n",
    "\n",
    "def get_date_item_ids(date, all_items):\n",
    "    return [i['id'] for i in all_items if get_acquired_date(i) == date]\n",
    "\n",
    "def get_ids_by_date(items):\n",
    "    acquired_dates = [get_acquired_date(item) for item in items]\n",
    "    unique_acquired_dates = set(acquired_dates)\n",
    "    \n",
    "    ids_by_date = dict((d, get_date_item_ids(d, items))\n",
    "                       for d in unique_acquired_dates)\n",
    "    return ids_by_date\n",
    "    \n",
    "ids_by_date = get_ids_by_date(item_list)\n",
    "pprint(ids_by_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ids_by_date[list(unique_acquired_dates)[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Submit Orders\n",
    "\n",
    "Now that we have the scene ids for each collect date, we can create the orders for each date. The output of each order is a single zip file that contains one composited scene and one composited UDM2.\n",
    "\n",
    "For this step we will just use the python api. See part 1 for a demonstration of how to use the CLI.\n",
    "\n",
    "##### Step 4.1: Build Order Toolchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_type = 'PSScene'\n",
    "bundle = 'analytic_sr_udm2'\n",
    "name = 'tutorial_order2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify tools\n",
    "\n",
    "# clip to AOI\n",
    "clip_tool = {'clip': {'aoi': test_aoi_geom}}\n",
    "\n",
    "# convert to NDVI\n",
    "bandmath_tool = {'bandmath': {\n",
    "    \"pixel_type\": \"32R\",\n",
    "    \"b1\": \"(b4 - b3) / (b4+b3)\"\n",
    "}}\n",
    "\n",
    "# composite\n",
    "composite_tool = {\n",
    "      \"composite\": {\n",
    "      }\n",
    "}\n",
    "\n",
    "tools = [clip_tool, bandmath_tool, composite_tool]\n",
    "pprint(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the order request\n",
    "# We will put this into a function so we can loop over all of our dates/image_IDs of interest.\n",
    "def build_order_request(ids):\n",
    "    orders_request = {\n",
    "        \"name\": name,\n",
    "        \"products\": [{\n",
    "            \"item_ids\": ids,\n",
    "            \"item_type\": item_type,\n",
    "            \"product_bundle\": bundle\n",
    "        }],\n",
    "        \"tools\": tools,\n",
    "        \"delivery\": {\n",
    "            \"single_archive\": True,\n",
    "            \"archive_filename\":\"{{name}}_{{order_id}}.zip\",\n",
    "            \"archive_type\":\"zip\"\n",
    "        },\n",
    "            \"notifications\": {\n",
    "                       \"email\": False\n",
    "        },\n",
    "    }\n",
    "    return orders_request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_of_order_requests = []\n",
    "\n",
    "for date in list(unique_acquired_dates):\n",
    "    ids = ids_by_date[date]\n",
    "    list_of_order_requests.append(build_order_request(ids))\n",
    "    \n",
    "print(list_of_order_requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4.2: Submit Orders\n",
    "\n",
    "In this section, for the sake of demonstration, we limit our orders to 2. Feel free to increase this limit if you want!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_limit = 2\n",
    "list_orders = []\n",
    "\n",
    "# Place the order\n",
    "for order_request in list_of_order_requests[:order_limit]:\n",
    "    async with Session() as sess:\n",
    "        cl = OrdersClient(sess)\n",
    "        order = await cl.create_order(order_request)\n",
    "    list_orders.append(order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the orders info\n",
    "list_orders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Download Orders\n",
    "\n",
    "##### Step 5.1: Wait Until Orders are Successful\n",
    "\n",
    "Before we can download the orders, they have to be prepared on the server.\n",
    "\n",
    "##### Step 5.2: Run Download\n",
    "\n",
    "For this step we will use the planet python orders API because we want to be able to download multiple orders at once, something the CLI does not yet support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we have multiple Order IDs, let's get them into a list\n",
    "order_id_list = []\n",
    "\n",
    "for order in list_orders:\n",
    "    order_id = order[\"id\"]\n",
    "    order_id_list.append(order_id)\n",
    "    \n",
    "print(order_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish the directory where we want to download the data\n",
    "data_dir = os.path.join('data', 'use_case_1')\n",
    "\n",
    "# make the download directory if it doesn't exist\n",
    "Path(data_dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we will make sure the orders have reached a downloadable state. \n",
    "# Then, we will download the orders.\n",
    "# This may take several minutes.\n",
    "\n",
    "async with Session() as sess:\n",
    "    client = OrdersClient(sess)\n",
    "    await asyncio.gather(\n",
    "        client.wait(order_id_list[0]),\n",
    "        client.wait(order_id_list[1]),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async with Session() as sess:\n",
    "    client = OrdersClient(sess)\n",
    "    await asyncio.gather(\n",
    "        client.download_order(order_id_list[0], data_dir, client),\n",
    "        client.download_order(order_id_list[1], data_dir, client),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check our downloaded file locations\n",
    "!ls data/use_case_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = []\n",
    "def get_download_locations(download_dir, order_id_list):\n",
    "    for order_id in order_id_list:\n",
    "        manifest_file = os.path.join(download_dir, order_id, 'manifest.json')\n",
    "        with open(manifest_file, 'r') as src:\n",
    "            manifest = json.load(src)\n",
    "        location = [os.path.join(data_dir, order_id, f['path'])\n",
    "                     for f in manifest['files']]\n",
    "        locations.append(location)\n",
    "    return locations\n",
    "\n",
    "locations = get_download_locations(data_dir, order_id_list)\n",
    "\n",
    "# un-nest our locations object\n",
    "locations = list(chain.from_iterable(locations))\n",
    "\n",
    "pprint(locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Step 6: Unzip and Verify Orders\n",
    "\n",
    "In this step we will simply unzip the orders and view one of the ordered composite images.\n",
    "\n",
    "##### 6.1: Unzip Order\n",
    "\n",
    "In this section, we will unzip each order into a directory named after the downloaded zip file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip(filename):\n",
    "    location = Path(filename)\n",
    "    \n",
    "    zipdir = location.parent / location.stem\n",
    "    with ZipFile(location) as myzip:\n",
    "        myzip.extractall(zipdir)\n",
    "    return zipdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unzipped_files(zipdir):\n",
    "    filedir = zipdir / 'files'\n",
    "    filenames = os.listdir(filedir)\n",
    "    return [filedir / f for f in filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can un-zip all our files using the functions we defined above\n",
    "for i in locations:\n",
    "    zipdir = unzip(i)\n",
    "    file_paths = get_unzipped_files(zipdir)\n",
    "    pprint(file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.2: Verify Orders\n",
    "\n",
    "In this section we will view the orders manually in QGIS. In the next part of this tutorial, we will visualize the NDVI composite image with the UDM. But for now, we just want to make sure we got what we ordered.\n",
    "\n",
    "In the explorer, navigate to the data folder (should be `*/notebooks/analysis-ready-data/data/use_case_1/`). In any of the subfolders (named with the `order_id`), go into `files` and find the file named `composite.tif`). Drag that file into QGIS to visualize."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
