{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note***: This notebook uses a deprecated item type `PSOrthoTile` and an old version of our Planet Python Client. A new version of this notebook is currently being developed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crop Type Classification: CART L8 and PS\n",
    "\n",
    "This notebook is a continuation of [Crop Type Classification: CART\n",
    "](classify-cart.ipynb) in which we use Landsat 8 as well as the PS Orthotile to generate features for CART classification.\n",
    "\n",
    "This notebook demonstrates the following:\n",
    "1. Finding a Landsat 8 scene that overlaps a PS Orthotile\n",
    "1. Resampling Landsat 8 bands to match a PS Orthotile\n",
    "1. Loading, visualizing, and using bitwise logic to convert a Landsat 8 QA band to a mask\n",
    "1. Training a CART classifier using a combination of features from the PS Orthotile and Landsat 8 scene\n",
    "1. Quantifying the performance of the classifier on the training data (upper limit of performance on a new dataset)\n",
    "\n",
    "To enable this notebook, a lot of functionality was copied from [Crop Type Classification: CART\n",
    "](classify-cart.ipynb).\n",
    "\n",
    "This notebook only utilizes the train dataset and tests the classifier results on the train dataset again. Future improvements would include testing on the train dataset.\n",
    "\n",
    "\n",
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, OrderedDict\n",
    "import json\n",
    "import os\n",
    "import pathlib\n",
    "from subprocess import check_output, STDOUT, CalledProcessError\n",
    "import tempfile\n",
    "from xml.dom import minidom\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Datasets\n",
    "\n",
    "### PS Orthotile\n",
    "\n",
    "Identify and, if necessary, download files associated with PS Orthotile [210879_1558814_2016-07-25_0e16](https://api.planet.com/data/v1/item-types/PSOrthoTile/items/210879_1558814_2016-07-25_0e16/thumb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scene_id = '210879_1558814_2016-07-25_0e16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and, if necessary, create cart data directory\n",
    "train_folder = os.path.join('data', 'cart', '210879_1558814_2016-07-25_0e16')\n",
    "pathlib.Path(train_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# define data file filenames and ensure they exist\n",
    "train_files = {\n",
    "    'scene': os.path.join(train_folder, '210879_1558814_2016-07-25_0e16_BGRN_Analytic.tif'),\n",
    "    'metadata': os.path.join(train_folder, '210879_1558814_2016-07-25_0e16_BGRN_Analytic_metadata.xml'),\n",
    "    'udm': os.path.join(train_folder, '210879_1558814_2016-07-25_0e16_BGRN_DN_udm.tif'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-21 11:19:46,555 - FuturesSession - WARNING - `background_callback` is deprecated and will be removed in 1.0, use `hooks` instead\n",
      "2022-04-21 11:19:47,059 - FuturesSession - WARNING - `background_callback` is deprecated and will be removed in 1.0, use `hooks` instead\n",
      "2022-04-21 11:19:47,560 - FuturesSession - WARNING - `background_callback` is deprecated and will be removed in 1.0, use `hooks` instead\n",
      "{\"item\": \"210879_1558814_2016-07-25_0e16\", \"asset\": \"analytic_xml\", \"location\": \"/Users/mansi/Documents/Code/notebooks/jupyter-notebooks/crop-classification/data/cart/210879_1558814_2016-07-25_0e16/210879_1558814_2016-07-25_0e16_BGRN_Analytic_metadata.xml\"}\n",
      "{\"item\": \"210879_1558814_2016-07-25_0e16\", \"asset\": \"analytic\", \"location\": \"/Users/mansi/Documents/Code/notebooks/jupyter-notebooks/crop-classification/data/cart/210879_1558814_2016-07-25_0e16/210879_1558814_2016-07-25_0e16_BGRN_Analytic.tif\"}\n",
      "{\"item\": \"210879_1558814_2016-07-25_0e16\", \"asset\": \"udm\", \"location\": \"/Users/mansi/Documents/Code/notebooks/jupyter-notebooks/crop-classification/data/cart/210879_1558814_2016-07-25_0e16/210879_1558814_2016-07-25_0e16_BGRN_DN_udm.tif\"}\n"
     ]
    }
   ],
   "source": [
    "train_scene = train_files['scene']\n",
    "\n",
    "# First test if scene file exists. \n",
    "!test -f $train_scene\n",
    "\n",
    "# If not, use the Planet commandline tool to download the image, metadata, and udm. This may take some time.\n",
    "# This command assumes a bash shell, available in Unix-based operating systems.\n",
    "# Remove the \"--quiet\" option if you would like to see the download in progress\n",
    "!planet data download \\\n",
    "        --item-type PSOrthoTile \\\n",
    "        --dest $train_folder \\\n",
    "        --asset-type analytic,analytic_xml,udm \\\n",
    "        --string-in id $train_scene_id \\\n",
    "        --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/cart/210879_1558814_2016-07-25_0e16/210879_1558814_2016-07-25_0e16_BGRN_Analytic.tif\n",
      "data/cart/210879_1558814_2016-07-25_0e16/210879_1558814_2016-07-25_0e16_BGRN_Analytic_metadata.xml\n",
      "data/cart/210879_1558814_2016-07-25_0e16/210879_1558814_2016-07-25_0e16_BGRN_DN_udm.tif\n"
     ]
    }
   ],
   "source": [
    "for filename in train_files.values():\n",
    "    print(filename)\n",
    "    assert os.path.isfile(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/cart/210879_1558814_2016-07-25_0e16\n",
      "data/cart/210879_1558814_2016-07-25_0e16/210879_1558814_2016-07-25_0e16_BGRN_Analytic.tif\n",
      "data/cart/210879_1558814_2016-07-25_0e16/210879_1558814_2016-07-25_0e16_BGRN_Analytic_metadata.xml\n",
      "data/cart/210879_1558814_2016-07-25_0e16/210879_1558814_2016-07-25_0e16_BGRN_DN_udm.tif\n"
     ]
    }
   ],
   "source": [
    "# define data file filenames and ensure they exist\n",
    "train_folder = os.path.join('data', 'cart', '210879_1558814_2016-07-25_0e16')\n",
    "print(train_folder)\n",
    "\n",
    "train_files = {\n",
    "    'scene': os.path.join(train_folder, '210879_1558814_2016-07-25_0e16_BGRN_Analytic.tif'),\n",
    "    'metadata': os.path.join(train_folder, '210879_1558814_2016-07-25_0e16_BGRN_Analytic_metadata.xml'),\n",
    "    'udm': os.path.join(train_folder, '210879_1558814_2016-07-25_0e16_BGRN_DN_udm.tif'),\n",
    "}\n",
    "\n",
    "for filename in train_files.values():\n",
    "    print(filename)\n",
    "    assert os.path.isfile(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify Gold Dataset\n",
    "\n",
    "The CDL image is used as the gold dataset. The train CDL image is prepared in the `datasets-prepare-cdl` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "predata_folder = 'pre-data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_CDL_filename = os.path.join(predata_folder, 'CDL_2016_19_train.tif')\n",
    "assert os.path.isfile(train_CDL_filename)\n",
    "train_files['gold'] = train_CDL_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Landsat 8 Scene\n",
    "\n",
    "To find the Landsat 8 scene that corresponds to the PS Orthotile, we read the Orthotile footprint from the Orthotile metadata and save it as geojson for use in searching for Landsat 8 scenes in Planet Explorer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"type\": \"Feature\", \"properties\": {}, \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[[-93.2996742135984, 42.8124283547201], [-93.2884031844075, 42.8620211383699], [-93.2798112490354, 42.8609332770922], [-93.2788644582494, 42.8651022427014], [-93.2653124631091, 42.9247751952826], [-92.9938730936993, 42.9251238519476], [-92.9938879901107, 42.7739366737022], [-92.998428279149, 42.7544585222305], [-93.0062882869115, 42.7554571101588], [-93.0192213170212, 42.6999567940072], [-93.2991294841129, 42.6995987669915], [-93.2996742135984, 42.8124283547201]]]}}\n",
      "data/cart/210879_1558814_2016-07-25_0e16/aoi.geojson\n"
     ]
    }
   ],
   "source": [
    "# save Orthotile footprint as aoi geojson\n",
    "\n",
    "def get_footprint_coords(metadata_filename):\n",
    "    xmldoc = minidom.parse(metadata_filename)\n",
    "    fp_node = xmldoc.getElementsByTagName('ps:Footprint')[0]\n",
    "\n",
    "    # hone in on the footprint coordinates\n",
    "    # the coordinates are always specified in WGS84, which is also the\n",
    "    # geographic coordeinate system\n",
    "    coords_node = fp_node.getElementsByTagName('gml:coordinates')[0]\n",
    "    coords_str = coords_node.firstChild.data\n",
    "    \n",
    "    # coordinates entry is space-separated lat,lon\n",
    "    coords = [[float(c) for c in cs.split(',')] for cs in coords_str.split(' ')]\n",
    "    return coords\n",
    "\n",
    "def coords_to_feature(coords):\n",
    "    geom = {\n",
    "        \"type\": \"Polygon\",\n",
    "        \"coordinates\": [coords]\n",
    "        }\n",
    "    \n",
    "    feature = {\n",
    "        \"type\": \"Feature\",\n",
    "        \"properties\": {},\n",
    "        \"geometry\": geom}\n",
    "\n",
    "    return feature\n",
    "        \n",
    "def write_geojson(feature, filename):\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(json.dumps(feature))\n",
    "    \n",
    "    print(filename)\n",
    "        \n",
    "coords = get_footprint_coords(train_files['metadata'])\n",
    "feat = coords_to_feature(coords)\n",
    "print(json.dumps(feat))\n",
    "\n",
    "# save aoi and add to list of files\n",
    "aoi_filename = os.path.join(train_folder, 'aoi.geojson')\n",
    "write_geojson(feat, aoi_filename)\n",
    "train_files['aoi'] = aoi_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download Landsat Scene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In planet explorer, we upload the AOI as a filter geometry then filter the dates to July 1, 2016 to September 2, 2016. This query results in two Landsat 8 scenes, with [LC80260302016245LGN00](https://api.planet.com/data/v1/item-types/Landsat8L1G/items/LC80260302016245LGN00/thumb) standing out as the best scene to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "landsat_scene_id = 'LC80260302016245LGN00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "  \"features\": [],\r\n",
      "  \"type\": \"FeatureCollection\"\r\n",
      "}"
     ]
    }
   ],
   "source": [
    "# Learn more about Landsat8L1G this scene\n",
    "\n",
    "!planet data search --item-type Landsat8L1G --string-in id $landsat_scene_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and, if necessary, create cart data directory\n",
    "src_l8_folder = os.path.join('data', 'cart', 'LC80260302016245LGN00')\n",
    "pathlib.Path(src_l8_folder).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qa': 'data/cart/LC80260302016245LGN00/LC80260302016245LGN00_BQA.TIF',\n",
       " 'b2': 'data/cart/LC80260302016245LGN00/LC80260302016245LGN00_B2.TIF',\n",
       " 'b3': 'data/cart/LC80260302016245LGN00/LC80260302016245LGN00_B3.TIF',\n",
       " 'b4': 'data/cart/LC80260302016245LGN00/LC80260302016245LGN00_B4.TIF',\n",
       " 'b5': 'data/cart/LC80260302016245LGN00/LC80260302016245LGN00_B5.TIF'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define landsat 8 bands\n",
    "l8_filenames = {\n",
    "    'qa': 'LC80260302016245LGN00_BQA.TIF',\n",
    "    'b2': 'LC80260302016245LGN00_B2.TIF',\n",
    "    'b3': 'LC80260302016245LGN00_B3.TIF',\n",
    "    'b4': 'LC80260302016245LGN00_B4.TIF',\n",
    "    'b5': 'LC80260302016245LGN00_B5.TIF',\n",
    "#     'b6': 'LC80260302016245LGN00_B6.TIF',\n",
    "#     'b7': 'LC80260302016245LGN00_B7.TIF'\n",
    "}\n",
    "\n",
    "def abs_path_filenames(folder, filenames):\n",
    "    return dict([(k, os.path.join(folder, fn))\n",
    "                 for k, fn in filenames.items()])\n",
    "\n",
    "src_l8_files = abs_path_filenames(src_l8_folder, l8_filenames)\n",
    "src_l8_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LC80260302016245LGN00'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "landsat_scene_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "landsat_qa = src_l8_files['qa']\n",
    "\n",
    "# First test if scene file exists. \n",
    "!test -f $landsat_qa\n",
    "\n",
    "# If not, use the Planet commandline tool to download the image, metadata, and udm. This may take some time.\n",
    "# This command assumes a bash shell, available in Unix-based operating systems.\n",
    "# Remove the \"--quiet\" option if you would like to see the download in progress\n",
    "!planet data download \\\n",
    "        --item-type Landsat8L1G \\\n",
    "        --dest $src_l8_folder \\\n",
    "        --asset-type analytic_bqa,analytic_b2,analytic_b3,analytic_b4,analytic_b5 \\\n",
    "        --string-in id $landsat_scene_id \\\n",
    "        --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "!planet data download \\\n",
    "        --item-type Landsat8L1G \\\n",
    "        --asset-type analytic_bqa,analytic_b2,analytic_b3,analytic_b4,analytic_b5 \\\n",
    "        --string-in id 'LC80260302016245LGN00' \\\n",
    "        --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md                  datasets-prepare-cdl.ipynb\r\n",
      "classify-cart-l8-ps.ipynb  datasets-prepare.ipynb\r\n",
      "classify-cart.ipynb        \u001b[34mpre-data\u001b[m\u001b[m\r\n",
      "\u001b[34mdata\u001b[m\u001b[m                       segment-knn-tuning.ipynb\r\n",
      "datasets-identify.ipynb    segment-knn.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/cart/LC80260302016245LGN00/LC80260302016245LGN00_BQA.TIF\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [35]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m src_l8_files\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(filename)\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(filename)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for filename in src_l8_files.values():\n",
    "    print(filename)\n",
    "    assert os.path.isfile(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resample Landsat Scene to PS Orthotile\n",
    "\n",
    "To stack the Landsat 8 and PS Orthothile bands, the pixels must be the same size and line up spatially. To accomplish this, we resample the Landsat 8 scene to the Orthotile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions: crop, resample, and project an image\n",
    "\n",
    "# These use gdalwarp. for a description of gdalwarp command line options, see:\n",
    "# http://www.gdal.org/gdalwarp.html\n",
    "\n",
    "# projection is not required for our application, where the Landsat\n",
    "# scene and the PS Orthotile are projected to the same UTM Zone\n",
    "# but this code is kept here in case that changes\n",
    "# def gdalwarp_project_options(src_crs, dst_crs):\n",
    "#     return ['-s_srs', src_crs.to_string(), '-t_srs', dst_crs.to_string()]\n",
    "\n",
    "def gdalwarp_crop_options(bounds, crs):\n",
    "    xmin, ymin, xmax, ymax = [str(b) for b in bounds]\n",
    "    # -te xmin ymin xmax ymax\n",
    "    return ['-te', xmin, ymin, xmax, ymax]\n",
    "\n",
    "def gdalwarp_resample_options(width, height, technique='near'):\n",
    "    # for technique options, see: http://www.gdal.org/gdalwarp.html\n",
    "    return ['-ts', width, height, '-r', technique]\n",
    "\n",
    "def gdalwarp(input_filename, output_filename, options):\n",
    "    commands = _gdalwarp_commands(input_filename, output_filename, options)\n",
    "\n",
    "    # print error if one is encountered\n",
    "    # https://stackoverflow.com/questions/29580663/save-error-message-of-subprocess-command\n",
    "    try:\n",
    "        output = check_output(commands, stderr=STDOUT)\n",
    "    except CalledProcessError as exc:\n",
    "        print(exc.output)\n",
    "\n",
    "def _gdalwarp_commands(input_filename, output_filename, options):\n",
    "    commands = ['gdalwarp'] + options + \\\n",
    "               ['-overwrite',\n",
    "                input_filename,\n",
    "                output_filename]\n",
    "    print(' '.join(commands))\n",
    "    return commands\n",
    "\n",
    "def _test():\n",
    "    TEST_DST_SCENE = train_files['scene']\n",
    "    TEST_SRC_SCENE = src_l8_files['qa']\n",
    "\n",
    "    with rasterio.open(TEST_DST_SCENE, 'r') as dst:\n",
    "        with rasterio.open(TEST_SRC_SCENE, 'r') as src:\n",
    "#             print(gdalwarp_project_options(src.crs, dst.crs))\n",
    "            print(gdalwarp_crop_options(dst.bounds, dst.crs))\n",
    "            print(gdalwarp_resample_options(dst.width, dst.height))\n",
    "# _test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_l8_band(band_filename, dst_filename, out_filename, classified=False):\n",
    "    '''Project, crop, and resample landsat 8 band to match dst_filename image.'''\n",
    "    \n",
    "    # use 'near' resampling method for classified (e.g. qa) band,\n",
    "    # otherwise use 'cubic' method\n",
    "    method = 'near' if classified else 'cubic'\n",
    "    \n",
    "    with rasterio.open(band_filename, 'r') as src:\n",
    "        with rasterio.open(dst_filename, 'r') as dst:\n",
    "            # project\n",
    "            # proj_options = gdalwarp_project_options(src_crs, dst.crs)\n",
    "\n",
    "            # crop\n",
    "            crop_options = gdalwarp_crop_options(dst.bounds, dst.crs)\n",
    "\n",
    "            # resample\n",
    "            width, height = dst.shape\n",
    "            resample_options = gdalwarp_resample_options(str(width), str(height), method)\n",
    "\n",
    "            options = crop_options + resample_options\n",
    "            \n",
    "            # run gdalwarp\n",
    "            gdalwarp(band_filename, out_filename, options)\n",
    "\n",
    "\n",
    "def _test(delete=True):\n",
    "    TEST_DST_SCENE = train_files['scene']\n",
    "    TEST_SRC_SCENE = src_l8_files['qa']\n",
    "    \n",
    "    with tempfile.NamedTemporaryFile(suffix='.tif', delete=delete, dir='.') as out_file:\n",
    "        # create output\n",
    "        prepare_l8_band(TEST_SRC_SCENE, TEST_DST_SCENE, out_file.name, classified=True)\n",
    "\n",
    "        # check output\n",
    "        with rasterio.open(TEST_DST_SCENE, 'r') as dst:\n",
    "            with rasterio.open(out_file.name, 'r') as src:\n",
    "                assert dst.crs == src.crs, '{} != {}'.format(src.crs, dst.crs)\n",
    "                assert dst.bounds == src.bounds\n",
    "                assert dst.shape == src.shape\n",
    "# _test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_l8_bands(src_files, dst_folder, ps_scene):\n",
    "    dst_files = {}\n",
    "    for name, filename in src_l8_files.items():\n",
    "        # qa band is the only classified band\n",
    "        classified = name=='qa'\n",
    "        \n",
    "        dst_file = os.path.join(dst_folder, os.path.basename(filename))\n",
    "        prepare_l8_band(filename, ps_scene, dst_file, classified=classified)\n",
    "        dst_files[name] = dst_file\n",
    "    return dst_files\n",
    "\n",
    "def _test():\n",
    "    try:\n",
    "        out_folder = tempfile.mkdtemp()\n",
    "        dst_l8_files = prepare_l8_bands(src_l8_files, out_folder, train_files['scene'])\n",
    "        print(dst_l8_files)\n",
    "    finally:\n",
    "        del out_folder\n",
    "# _test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_l8_folder = 'data/cart/210879_1558814_2016-07-25_0e16/L8'\n",
    "\n",
    "if not os.path.isdir(train_l8_folder):\n",
    "    os.mkdir(train_l8_folder)\n",
    "    print(train_l8_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_l8_files = prepare_l8_bands(src_l8_files, train_l8_folder, train_files['scene'])\n",
    "train_l8_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Landsat 8 QA band\n",
    "\n",
    "We use the Landsat 8 QA band to mask out any 'bad' (quality issue, cloud, etc) pixels. To accomplish this, first we create functionality for dealing with any generic classified band, then we load the QA band, visualize it, and convert it to a mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions: visualizing a classified band as an image\n",
    "\n",
    "def plot_image(image, title, figsize=(10,10)):\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    image.imshow(ax)\n",
    "    ax.set_title(title)\n",
    "    ax.set_axis_off()\n",
    "        \n",
    "class ClassifiedImage(object):\n",
    "    def __init__(self, band, labels=None):\n",
    "        self.band = band\n",
    "        self.labels = labels\n",
    "\n",
    "    def imshow(self, ax, cmap='rainbow'):\n",
    "        \"\"\"Show classified band with colormap normalization and color legend.\n",
    "        \n",
    "        Alters ax in place.\n",
    "\n",
    "        possible cmaps ref: https://matplotlib.org/examples/color/colormaps_reference.html\n",
    "        \"\"\"\n",
    "        class_norm = _ClassNormalize(self.band)\n",
    "        im = ax.imshow(self.band, cmap=cmap, norm=class_norm)\n",
    "\n",
    "        try:\n",
    "            # add class label legend\n",
    "            # https://stackoverflow.com/questions/25482876\n",
    "            # /how-to-add-legend-to-imshow-in-matplotlib\n",
    "            color_mapping = class_norm.mapping\n",
    "            colors = [im.cmap(color_mapping[k])\n",
    "                      for k in self.labels.keys()]\n",
    "            labels = self.labels.values()\n",
    "\n",
    "            # https://matplotlib.org/users/legend_guide.html\n",
    "            # tag: #creating-artists-specifically-for-adding-to-the-legend-aka-proxy-artists\n",
    "            patches = [mpatches.Patch(color=c, label=l)\n",
    "                       for c,l in zip(colors, labels)]\n",
    "\n",
    "            ax.legend(handles=patches, bbox_to_anchor=(1, 1),\n",
    "                      loc='upper right', borderaxespad=0.)\n",
    "        except AttributeError:\n",
    "            # labels not specified\n",
    "            pass\n",
    "\n",
    "\n",
    "# https://matplotlib.org/users/colormapnorms.html#custom-normalization-two-linear-ranges\n",
    "class _ClassNormalize(colors.Normalize):\n",
    "    \"\"\"Matplotlib colormap normalizer for a classified band.\n",
    "    \n",
    "    __init__ and __call__ are the minimum required methods.\n",
    "    \"\"\"\n",
    "    def __init__(self, arry):\n",
    "        # get unique unmasked values\n",
    "        values = [v for v in np.unique(arry)\n",
    "                  if not isinstance(v, np.ma.core.MaskedConstant)]\n",
    "\n",
    "        color_ticks = np.array(range(len(values)), dtype=np.float) / (len(values) - 1)\n",
    "        self._mapping = dict((v, ct)\n",
    "                            for v, ct in zip(values, color_ticks))\n",
    "        \n",
    "        # Initialize base Normalize instance\n",
    "        vmin = 0\n",
    "        vmax = 1\n",
    "        clip = False\n",
    "        colors.Normalize.__init__(self, vmin, vmax, clip)\n",
    "    \n",
    "    def __call__(self, arry, clip=None):\n",
    "        # round array back to ints for logical comparison\n",
    "        arry = np.around(arry)\n",
    "        new_arry = arry.copy()\n",
    "        for k, v in self._mapping.items():\n",
    "            new_arry[arry==k] = v\n",
    "        return new_arry\n",
    "    \n",
    "    @property\n",
    "    def mapping(self):\n",
    "        return self._mapping\n",
    "\n",
    "def _test():\n",
    "#     classified_band = np.array(range(4)).reshape((2,2))\n",
    "    classified_band = np.array([0, 1, 2, 28, 30, 64, 66, 92, 94], dtype=np.uint8).reshape((3,3))\n",
    "    print(classified_band)\n",
    "    labels = OrderedDict((v, str(v)) for v in np.unique(classified_band))\n",
    "    classified_image = ClassifiedImage(band=classified_band, labels=labels)\n",
    "    plot_image(classified_image, title='Test', figsize=(4,4))\n",
    "# _test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_classified_band(class_band, class_labels=None, cmap='rainbow',\n",
    "                         title='Class Labels', figdim=10):\n",
    "    fig = plt.figure(figsize=(figdim, figdim))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    imshow_class_band(ax, class_band, class_labels, cmap=cmap)\n",
    "    ax.set_title(title)\n",
    "    ax.set_axis_off()\n",
    "\n",
    "def imshow_class_band(ax, class_band, class_labels=None, cmap='rainbow'):\n",
    "    \"\"\"Show classified band with colormap normalization and color legend. Alters ax in place.\n",
    "    \n",
    "    possible cmaps ref: https://matplotlib.org/examples/color/colormaps_reference.html\n",
    "    \"\"\"\n",
    "    class_norm = _ClassNormalize(class_band)\n",
    "    im = ax.imshow(class_band, cmap=cmap, norm=class_norm)\n",
    "\n",
    "    try:\n",
    "        # add class label legend\n",
    "        # https://stackoverflow.com/questions/25482876\n",
    "        # /how-to-add-legend-to-imshow-in-matplotlib\n",
    "        color_mapping = class_norm.mapping\n",
    "        colors = [im.cmap(color_mapping[k]) for k in class_labels.keys()]\n",
    "        labels = class_labels.values()\n",
    "\n",
    "        # https://matplotlib.org/users/legend_guide.html\n",
    "        # tag: #creating-artists-specifically-for-adding-to-the-legend-aka-proxy-artists\n",
    "        patches = [mpatches.Patch(color=c, label=l) for c,l in zip(colors, labels)]\n",
    "\n",
    "        ax.legend(handles=patches, bbox_to_anchor=(1, 1), loc='upper right', borderaxespad=0.)\n",
    "    except AttributeError:\n",
    "        # class_labels not specified\n",
    "        pass\n",
    "\n",
    "# https://matplotlib.org/users/colormapnorms.html#custom-normalization-two-linear-ranges\n",
    "class _ClassNormalize(colors.Normalize):\n",
    "    \"\"\"Matplotlib colormap normalizer for a classified band.\n",
    "    \n",
    "    Inspired by https://matplotlib.org/users/colormapnorms.html#custom-normalization-two-linear-ranges\n",
    "    \"\"\"\n",
    "    def __init__(self, arry):\n",
    "        # get unique unmasked values\n",
    "        values = [v for v in np.unique(arry)\n",
    "                  if not isinstance(v, np.ma.core.MaskedConstant)]\n",
    "\n",
    "        # map unique values to points in the range 0-1\n",
    "        color_ticks = np.array(range(len(values)), dtype=np.float) / (len(values) - 1)\n",
    "        self._mapping = dict((v, ct) for v, ct in zip(values, color_ticks))\n",
    "        \n",
    "        # Initialize base Normalize instance\n",
    "        vmin = 0\n",
    "        vmax = 1\n",
    "        clip = False\n",
    "        colors.Normalize.__init__(self, vmin, vmax, clip)\n",
    "    \n",
    "    def __call__(self, arry, clip=None):\n",
    "        '''Create classified representation of arry for display.'''\n",
    "        # round array back to ints for logical comparison\n",
    "        arry = np.around(arry)\n",
    "        new_arry = arry.copy()\n",
    "        for k, v in self._mapping.items():\n",
    "            new_arry[arry==k] = v\n",
    "        return new_arry\n",
    "    \n",
    "    @property\n",
    "    def mapping(self):\n",
    "        '''property required for colors.Normalize classes\n",
    "        \n",
    "        We update the _mapping property in __init__ and __call__ and just\n",
    "        return that property here.\n",
    "        '''\n",
    "        return self._mapping\n",
    "    \n",
    "# test out classified band visualization\n",
    "test_classified_band = np.array(range(9)).reshape((3,3))\n",
    "plot_classified_band(test_classified_band, title='Test Classified Band', figdim=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functionality specific to the QA Band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for working with large images and memory limitations\n",
    "\n",
    "def _read_window(filename, window):\n",
    "    with rasterio.open(filename, 'r') as src:\n",
    "        return src.read(window=window)\n",
    "\n",
    "def decimated(arry, num=8):\n",
    "    return arry[::num, ::num].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_band(band_filename, window=None):\n",
    "    return _read_window(band_filename, window)[0,...]\n",
    "\n",
    "def get_qa_labels(binary_band):\n",
    "    return OrderedDict((v, '{0:b}'.format(v).zfill(16))\n",
    "                       for v in np.unique(binary_band))\n",
    "\n",
    "def _test():\n",
    "    qa = decimated(load_band(train_l8_files['qa']), num=1)\n",
    "    qa_labels = get_qa_labels(qa)\n",
    "    qa_classified_band = ClassifiedImage(band=qa, labels=qa_labels)\n",
    "#     plot_image(qa_classified_band, title='QA Band', figsize=(7,7))\n",
    "    plot_classified_band(qa, class_labels=qa_labels, title='QA Band', figdim=7)\n",
    "_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_to_mask(qa_array):\n",
    "    \"\"\"Generate mask from L8 QA band.\n",
    "    \n",
    "    Pre-Collection:\n",
    "    The description for the pre-collection QA band is no longer hosted by USGS.\n",
    "    We pull it from plcompositor, information encoded in the bits in the QA is from:\n",
    "    https://github.com/planetlabs/plcompositor/blob/master/src/landsat8cloudquality.cpp#L28\n",
    "    \n",
    "    For the single bits (0, 1, 2, and 3):\n",
    "    0 = No, this condition does not exist\n",
    "    1 = Yes, this condition exists.\n",
    "    \n",
    "    The double bits (4-5, 6-7, 8-9, 10-11, 12-13, and 14-15)\n",
    "    represent levels of confidence that a condition exists:\n",
    "    00 = Algorithm did not determine the status of this condition\n",
    "    01 = Algorithm has low confidence that this condition exists \n",
    "         (0-33 percent confidence)\n",
    "    10 = Algorithm has medium confidence that this condition exists \n",
    "         (34-66 percent confidence)\n",
    "    11 = Algorithm has high confidence that this condition exists \n",
    "         (67-100 percent confidence).\n",
    "\n",
    "     Mask    Meaning\n",
    "    0x0001 - Designated Fill\n",
    "    0x0002 - Dropped Frame\n",
    "    0x0004 - Terrain Occlusion\n",
    "    0x0008 - Reserved\n",
    "    0x0030 - Water Confidence\n",
    "    0x00c0 - Reserved for cloud shadow\n",
    "    0x0300 - Vegitation confidence\n",
    "    0x0c00 - Show/ice Confidence\n",
    "    0x3000 - Cirrus Confidence\n",
    "    0xc000 - Cloud Confidence\n",
    "    \n",
    "    Collection 1:\n",
    "    \n",
    "    The description for the information encoded in the bits in the QA is from:\n",
    "    https://landsat.usgs.gov/collectionqualityband\n",
    "    \n",
    "    Bit 0: designated fill\n",
    "    Bit 1: terrain occlusion\n",
    "    Bits 2/3: radiometric saturation\n",
    "    Bit 4: cloud\n",
    "    Bits 5/6: cloud confidence\n",
    "    Bits 7/8: cloud shadow confidence\n",
    "    Bits 9/10: snow/ice confidence\n",
    "    Bits 11/12: cirr\n",
    "    \"\"\"\n",
    "    # check for absolute or >= med confidence of any condition\n",
    "    test_bits = int('1010101000001111',2)\n",
    "    bit_matches = qa_array & test_bits # bit-wise logical AND operator\n",
    "    return bit_matches != 0 # mask any pixels that match test bits\n",
    "\n",
    "def _test():\n",
    "    mask = qa_to_mask(load_band(train_l8_files['qa']))\n",
    "    print('{}/{} ({:0.1f}%) masked'.format(mask.sum(), mask.size,\n",
    "                                           (100.0 * mask.sum())/mask.size))\n",
    "_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 0.4% masked bodes well for the usefulness of this Landsat 8 scene!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Landsat 8 and PS Orthotile Visual Images\n",
    "\n",
    "Before we jump into training the classifier, we want to visualize the Landsat 8 and PS Orthotile RGB images to get a feel for what they show.\n",
    "\n",
    "To do this, we first create classes that load and store the PS and L8 analytic images, (PSImage and L8Image, respectively). We then create a class that displays an RGB image, taking care of the necessary scaling and masking. We then use these classes to visualize the PS and L8 RGB images.\n",
    "\n",
    "### PS Orthotile Visual Image\n",
    "\n",
    "#### Mask from UDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_udm_labels(binary_band):\n",
    "    return OrderedDict((v, '{0:b}'.format(v).zfill(7))\n",
    "                       for v in np.unique(binary_band))\n",
    "\n",
    "def udm_to_mask(udm_array):\n",
    "    '''Create a mask from the udm.\n",
    "    \n",
    "    The description for the information encoded in the bits in the UDM is from:\n",
    "    https://www.planet.com/docs/spec-sheets/sat-imagery/\n",
    "    section 7.2\n",
    "    \n",
    "    Bit 0: blackfill\n",
    "    Bit 1: cloud covered\n",
    "    Bit 2: missing or suspect data in Blue band\n",
    "    Bit 3: missing or suspect data in Green band\n",
    "    Bit 4: missing or suspect data in Red band\n",
    "    Bit 6: missing or suspect data in NIR band\n",
    "\n",
    "    Pixels with no issues have all bits set to 0, therefore their values are zero.    \n",
    "    ''' \n",
    "    return udm_array != 0\n",
    "\n",
    "def _test():\n",
    "    udm = load_band(train_files['udm'])\n",
    "    mask = udm_to_mask(udm)\n",
    "    print('{}/{} ({:0.0f}%) masked'.format(mask.sum(), mask.size,\n",
    "                                          (100.0 * mask.sum())/mask.size))\n",
    "\n",
    "    udm_dec = decimated(udm, 32)\n",
    "    udm_labels = get_udm_labels(udm_dec)\n",
    "    udm_image = ClassifiedImage(band=udm_dec, labels=udm_labels)\n",
    "    plot_image(udm_image, title='UDM Band', figsize=(5,5))\n",
    "_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visual RGB\n",
    "\n",
    "Create a few classes that store the analytic and visual images, to simplify processing and visualizing the PS Orthotile image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PSBands = namedtuple('PSBands', 'b, g, r, nir')\n",
    "\n",
    "class PSImage(object):    \n",
    "    def __init__(self, scene_filename, udm_filename, window=None):\n",
    "        self.scene_filename = scene_filename\n",
    "        self.udm_filename = udm_filename\n",
    "        self.window = window\n",
    "\n",
    "        self._bands = self._load_bands()\n",
    "        \n",
    "    def _load_bands(self):\n",
    "        \"\"\"Loads a 4-band BGRNir Planet Image file as a list of masked bands.\n",
    "\n",
    "        The masked bands share the same mask, so editing one band mask will\n",
    "        edit them all.\n",
    "        \"\"\"\n",
    "        with rasterio.open(self.scene_filename, 'r') as src:\n",
    "            b, g, r, nir = src.read(window=self.window)\n",
    "            bands = PSBands(b=b, g=g, r=r, nir=nir)\n",
    "\n",
    "        with rasterio.open(self.udm_filename, 'r') as src:\n",
    "            udm = src.read(window=self.window)[0,...]\n",
    "\n",
    "        mask = udm_to_mask(udm)\n",
    "        return PSBands(*[np.ma.array(b, mask=mask) for b in bands])\n",
    "\n",
    "    def rgb_bands(self):\n",
    "        return [self._bands.r, self._bands.g, self._bands.b]\n",
    "        \n",
    "    @property\n",
    "    def mask(self):\n",
    "        return self._bands[0].mask\n",
    "    \n",
    "    @property\n",
    "    def bands(self):\n",
    "        return self._bands\n",
    "\n",
    "\n",
    "def check_mask(img):\n",
    "    band_mask = img.mask\n",
    "    return '{}/{} ({:0.0f}%) masked'.format(band_mask.sum(), band_mask.size,\n",
    "                                            (100.0 * band_mask.sum())/band_mask.size)\n",
    "\n",
    "def _test():\n",
    "    window = ((500,1500),(500,1500))\n",
    "    print(check_mask(PSImage(train_files['scene'], train_files['udm'], window=window)))\n",
    "\n",
    "    window = None\n",
    "    print(check_mask(PSImage(train_files['scene'], train_files['udm'], window=window)))\n",
    "# _test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions: displaying an rgb image\n",
    "\n",
    "def _linear_scale(ndarray, old_min, old_max, new_min, new_max):\n",
    "    \"\"\"Linear scale from old_min to new_min, old_max to new_max.\n",
    "    \n",
    "    Values below min/max are allowed in input and output.\n",
    "    Min/Max values are two data points that are used in the linear scaling.\n",
    "    \"\"\"\n",
    "    #https://en.wikipedia.org/wiki/Normalization_(image_processing)\n",
    "    return (ndarray - old_min) * (new_max - new_min) / (old_max - old_min) + new_min\n",
    "# print(linear_scale(np.array([1,2,10,100,256,2560, 2660]), 2, 2560, 0, 256))\n",
    "\n",
    "\n",
    "class RGBImage(object):\n",
    "    def __init__(self, bands):\n",
    "        # bands: list of masked bands in RGB order\n",
    "        # masked bands should share the same mask\n",
    "        assert len(bands) == 3\n",
    "        self.bands = bands\n",
    "\n",
    "    def imshow(self, ax, alpha=True):\n",
    "        \"\"\"Show RGB image with option of convering mask to alpha.\n",
    "        \n",
    "        Alters ax in place.\n",
    "\n",
    "        \"\"\"\n",
    "        ax.imshow(self.bands_to_display(alpha=alpha))\n",
    "        \n",
    "\n",
    "    def _mask_to_alpha(self):\n",
    "        band = np.atleast_3d(self.bands[0])\n",
    "        alpha = np.zeros_like(band)\n",
    "        alpha[~band.mask] = 1\n",
    "        return alpha\n",
    "\n",
    "    def _percentile(self, percentile):\n",
    "        return np.percentile(np.concatenate([b.compressed() for b in self.bands]),\n",
    "                             percentile)\n",
    "\n",
    "    def bands_to_display(self, alpha=False):\n",
    "        \"\"\"Converts bands to a normalized, 3-band 3d numpy array for display.\"\"\"  \n",
    "\n",
    "        old_min = self._percentile(2)\n",
    "        old_max = self._percentile(98)\n",
    "        new_min = 0\n",
    "        new_max = 1\n",
    "        scaled = [np.clip(_linear_scale(b.astype(np.float),\n",
    "                                        old_min, old_max,\n",
    "                                        new_min, new_max),\n",
    "                          new_min, new_max)\n",
    "                  for b in self.bands]\n",
    "\n",
    "        filled = [b.filled(fill_value=new_min) for b in scaled]\n",
    "\n",
    "        if alpha:\n",
    "            filled.append(self._mask_to_alpha())\n",
    "\n",
    "        return np.dstack(filled)\n",
    "\n",
    "def _test():\n",
    "    img = PSImage(train_files['scene'], train_files['udm'], window=None)\n",
    "    rgb_image = RGBImage([decimated(b) for b in img.rgb_bands()])\n",
    "    plot_image(rgb_image, title='PS RGB', figsize=(6,6))\n",
    "# _test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Landsat 8 Visual Image\n",
    "\n",
    "Create a class that stores the analytic Landsat 8 image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L8Bands = namedtuple('L8Bands', 'b2, b3, b4, b5')\n",
    "        \n",
    "class L8Image(object):\n",
    "    def __init__(self, band_filenames, qa_filename, window=None):\n",
    "        self.band_filenames = band_filenames\n",
    "        self.qa_filename = qa_filename\n",
    "        self.window = window\n",
    "\n",
    "        self._bands = self._load_bands()\n",
    "    \n",
    "    def _load_mask(self):\n",
    "        qa = self._read_band(self.qa_filename)\n",
    "        return qa_to_mask(qa)\n",
    "        \n",
    "    def _load_bands(self):\n",
    "        def _try_read_band(band_name, mask):\n",
    "            try:\n",
    "                filename = self.band_filenames[band_name]\n",
    "                band_arry = self._read_band(filename)\n",
    "                band = np.ma.array(band_arry, mask=mask)\n",
    "            except KeyError:\n",
    "                # band_name not a key in band_filenames\n",
    "                band = None\n",
    "            return band\n",
    "\n",
    "        mask = self._load_mask()\n",
    "        return L8Bands(*[_try_read_band(band_name, mask)\n",
    "                         for band_name in L8Bands._fields])\n",
    "    \n",
    "    def _read_band(self, filename):\n",
    "        with rasterio.open(filename, 'r') as src:\n",
    "            band = src.read(window=self.window)[0,...]\n",
    "        return band\n",
    "    \n",
    "    def rgb_bands(self):\n",
    "        rgb_bands = [self._bands.b4, self._bands.b3, self._bands.b2]\n",
    "        return rgb_bands\n",
    "\n",
    "    @property\n",
    "    def mask(self):\n",
    "        return self._bands[0].mask\n",
    "    \n",
    "    @property\n",
    "    def bands(self):\n",
    "        return self._bands\n",
    "\n",
    "def _test():\n",
    "    img = L8Image(train_l8_files, train_l8_files['qa'], window=None)\n",
    "    rgb_image = RGBImage([decimated(b) for b in img.rgb_bands()])\n",
    "    plot_image(rgb_image, title='L8 RGB', figsize=(6,6))\n",
    "_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Comparison\n",
    "\n",
    "Load the PS Orthotile and Landsat 8 images, then compare the visual RGB representation of those images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_rgb_comparison(ps_files, l8_files, figsize=(15,15)):\n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2,\n",
    "                                   sharex=True, sharey=True,\n",
    "                                   figsize=figsize)\n",
    "    for ax in (ax1, ax2):\n",
    "        ax.set_adjustable('box-forced')\n",
    "\n",
    "    # PS\n",
    "    img = PSImage(ps_files['scene'], ps_files['udm'], window=None)\n",
    "    ps_rgb_image = RGBImage([decimated(b) for b in img.rgb_bands()])\n",
    "    ps_rgb_image.imshow(ax1)\n",
    "    ax1.set_title('PS')\n",
    "\n",
    "    # L8\n",
    "    rgb_bandnames = ['b4', 'b3', 'b2']\n",
    "    rgb_files = dict([(k, l8_files[k]) for k in rgb_bandnames])\n",
    "    l8_img = L8Image(rgb_files, l8_files['qa'], window=None)\n",
    "    l8_rgb_image = RGBImage([decimated(b) for b in l8_img.rgb_bands()])\n",
    "    l8_rgb_image.imshow(ax2)\n",
    "    ax2.set_title('L8')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "plot_rgb_comparison(train_files, train_l8_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features from Images\n",
    "\n",
    "To classify the image, we first create 'feature bands', which are bands that hold each of the classification features, from the L8 and PS analytic images. An important step to creating the feature bands is ensuring that any pixel that is masked in either image is masked in the feature bands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_bands(ps_img, l8_img):\n",
    "    \"\"\"Prepare bands representing pixel features and provide feature names.\n",
    "    \n",
    "    Takes as input NamedBands\n",
    "    Returns (1) tuple of bands representing features and (2) feature names\n",
    "    \"\"\"  \n",
    "    # not copying these bands to minimize memory footprints\n",
    "    features = (ps_img.bands.b, ps_img.bands.g, ps_img.bands.r, ps_img.bands.nir,\n",
    "                l8_img.bands.b2, l8_img.bands.b3, l8_img.bands.b4, l8_img.bands.b5)\n",
    "    \n",
    "    new_mask = ps_img.mask | l8_img.mask\n",
    "    for band in features:\n",
    "        band.mask = new_mask\n",
    "\n",
    "    feature_names = ('PSBlue', 'PSGreen', 'PSRed', 'PSNIR',\n",
    "                     'L8B2', 'L8B3', 'L8B4', 'L8B5')\n",
    "    return features, feature_names\n",
    "\n",
    "\n",
    "def display_feature_bands(bands, names):\n",
    "    # for this notebook, we know there are 8 features and we will use that\n",
    "    # knowledge to side-step some logic in organizing subplots\n",
    "    assert len(bands) == 8 \n",
    "    \n",
    "    fig, subplot_axes = plt.subplots(nrows=4, ncols=2,\n",
    "                                     sharex=True, sharey=True,\n",
    "                                     figsize=(10,10))\n",
    "    axes = subplot_axes.flat[:-1]\n",
    "    delaxis = subplot_axes.flat[-1]\n",
    "    fig.delaxes(delaxis)\n",
    "    for ax, band, name in zip(axes, bands, names):\n",
    "        ax.set_adjustable('box-forced')\n",
    "        ax.axis('off')\n",
    "\n",
    "        pcm = ax.imshow(band, alpha=True)\n",
    "        ax.set_title(name)\n",
    "        fig.colorbar(pcm, ax=ax,\n",
    "                     pad=0.05, shrink=0.9)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "def _test():\n",
    "#     window = ((500,1500),(500,1500))\n",
    "    window = None\n",
    "\n",
    "    ps_img = PSImage(train_files['scene'], train_files['udm'], window=window)\n",
    "    l8_img = L8Image(train_l8_files, train_l8_files['qa'], window=window)\n",
    "    feat_bands, feat_names = build_feature_bands(ps_img, l8_img)\n",
    "    display_feature_bands(feat_bands, feat_names)\n",
    "_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify Image\n",
    "\n",
    "In this section, we load the labels and train the classifier using the feature bands, then run the prediction on the train feature bands to see how well the features differentiate the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from classify-cart.ipynb\n",
    "\n",
    "CLASS_LABELS = {1: 'corn', 5: 'soybeans'}\n",
    "\n",
    "def load_gold_class_band(gold_filename, class_labels=None, window=None, fill_value=0):\n",
    "    gold_class_band = _read_window(gold_filename, window)[0,...]\n",
    "    \n",
    "    try:\n",
    "        # mask pixels with a value not in class_labels\n",
    "        masks = [gold_class_band == val for val in class_labels.keys()]\n",
    "        mask = np.any(np.dstack(masks), axis=2)\n",
    "        mask = ~mask\n",
    "        \n",
    "        masked_band = np.ma.array(np.ma.array(gold_class_band, mask=mask)\\\n",
    "                                      .filled(fill_value=fill_value),\n",
    "                                  mask=mask)\n",
    "    except AttributeError:\n",
    "        # mask nothing\n",
    "        null_mask = np.zeros(gold_class_band.shape, dtype=np.bool)\n",
    "        masked_band = np.ma.array(gold_class_band, mask=null_mask)\n",
    "\n",
    "    return masked_band\n",
    "\n",
    "def _test():\n",
    "    labels = CLASS_LABELS\n",
    "    gold_band = load_gold_class_band(train_files['gold'], class_labels=labels)\n",
    "\n",
    "    gold_dec = decimated(gold_band)\n",
    "    gold_image = ClassifiedImage(band=gold_dec, labels=labels)\n",
    "    plot_image(gold_image, title='Gold Band', figsize=(5,5))\n",
    "# _test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from classify-cart.ipynb\n",
    "def to_X(feature_bands):\n",
    "    \"\"\"Convert feature_bands (tuple of bands) to 2d array for working with classifier.\n",
    "    \"\"\"\n",
    "    return np.stack([f.compressed() for f in feature_bands], # exclude masked pixels\n",
    "                     axis=1)\n",
    "\n",
    "def to_y(classified_band):\n",
    "    return classified_band.compressed()\n",
    "\n",
    "def classified_band_from_y(y, band_mask):\n",
    "    class_band = np.ma.array(np.zeros(band_mask.shape),\n",
    "                             mask=band_mask.copy())\n",
    "    class_band[~class_band.mask] = y\n",
    "    return class_band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features\n",
    "ps_img = PSImage(train_files['scene'],\n",
    "                 train_files['udm'],\n",
    "                 window=window)\n",
    "l8_img = L8Image(train_l8_files,\n",
    "                 train_l8_files['qa'],\n",
    "                 window=window)\n",
    "feat_bands, _ = build_feature_bands(ps_img, l8_img)\n",
    "X = to_X(feat_bands)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare labels\n",
    "labels = CLASS_LABELS\n",
    "gold_band = load_gold_class_band(train_files['gold'],\n",
    "                                 class_labels=labels,\n",
    "                                 window=window)\n",
    "gold_band.mask = feat_bands[0].mask #get_mask(feat_bands)\n",
    "y = to_y(gold_band)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train classifier on PS + L8 features\n",
    "\n",
    "By default, the classifier is loaded from the cache to speed up processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_cache_file = os.path.join('pre-data', 'classify-cart-l8-clf.sav')\n",
    "\n",
    "load_clf = True\n",
    "if load_clf:\n",
    "    assert os.path.isfile(clf_cache_file)\n",
    "    assert X.shape == (49273735, 8)\n",
    "    assert y.shape == (49273735,)\n",
    "    clf = pickle.load(open(clf_cache_file, 'rb'))\n",
    "else:\n",
    "    clf = DecisionTreeClassifier(random_state=0, max_depth=5)\n",
    "    clf.fit(X, y)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optionally save clf model\n",
    "save_clf = False\n",
    "if save_clf:\n",
    "    pickle.dump(clf, open(clf_cache_file, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Prediction on Train Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run prediction on train features\n",
    "y_pred = clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display classification results\n",
    "def plot_predicted_vs_truth(predicted_class_band, gold_class_band,\n",
    "                            class_labels=None, figsize=(15,15)):\n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2,\n",
    "                                   sharex=True, sharey=True,\n",
    "                                   figsize=figsize)\n",
    "    for ax in (ax1, ax2):\n",
    "        ax.set_adjustable('box-forced')\n",
    "    \n",
    "    pred_img = ClassifiedImage(band=predicted_class_band, labels=CLASS_LABELS)\n",
    "    pred_img.imshow(ax1)\n",
    "    ax1.set_title('Classifier Predicted Classes')\n",
    "\n",
    "    gold_img = ClassifiedImage(band=gold_class_band, labels=CLASS_LABELS)\n",
    "    gold_img.imshow(ax2)\n",
    "    ax2.set_title('Gold Dataset Classes')\n",
    "    plt.tight_layout()\n",
    "\n",
    "pred_band = classified_band_from_y(y_pred, feat_bands[0].mask)\n",
    "plot_predicted_vs_truth(pred_band, gold_band, class_labels=CLASS_LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantify Performance on Train Features\n",
    "\n",
    "Quantify the performance on the train features, which represents the upper limit for classification accuracy when a prediction is run on a new image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y,\n",
    "                            y_pred,\n",
    "                            target_names=['neither', 'corn', 'soybean']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier performance for classifying a pixel as neither, corn, or soybean, calculated on the train features, is an f1-score of 0.84."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare PS to PS + L8\n",
    "\n",
    "How does the accuracy of classification using PS + L8 compare to the accuracy using just PS?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ps_feature_bands(ps_img):\n",
    "    # not copying these bands to minimize memory footprints\n",
    "    features = (ps_img.bands.b, ps_img.bands.g, ps_img.bands.r, ps_img.bands.nir)\n",
    "    feature_names = ('PSBlue', 'PSGreen', 'PSRed', 'PSNIR')\n",
    "    return features, feature_names\n",
    "\n",
    "ps_feature_bands = (ps_img.bands.b, ps_img.bands.g, ps_img.bands.r, ps_img.bands.nir)\n",
    "ps_X = to_X(ps_feature_bands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train classifier on PS features\n",
    "\n",
    "By default, the classifier is loaded from the cache to speed up processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_clf_cache_file = os.path.join('pre-data', 'classify-cart-l8-ps-clf.sav')\n",
    "\n",
    "load_ps_clf = True\n",
    "if load_ps_clf:\n",
    "    assert os.path.isfile(ps_clf_cache_file)\n",
    "    assert ps_X.shape == (49273735, 4)\n",
    "    assert y.shape == (49273735,)\n",
    "    ps_clf = pickle.load(open(ps_clf_cache_file, 'rb'))\n",
    "else:\n",
    "    ps_clf = DecisionTreeClassifier(random_state=0, max_depth=5)\n",
    "    ps_clf.fit(ps_X, y)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optionally save ps clf model\n",
    "save_ps_clf = False\n",
    "if save_ps_clf:\n",
    "    pickle.dump(ps_clf, open(ps_clf_cache_file, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run prediction on train features\n",
    "ps_y_pred = ps_clf.predict(ps_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y,\n",
    "                            ps_y_pred,\n",
    "                            target_names=['neither', 'corn', 'soybean']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier performance for classifying a pixel as neither, corn, or soybean, calculated on the train features using just PS data, is an f1-score of 0.80, down from 0.84 when using PS and L8 data together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
